{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shapely\n",
    "import geofileops as gfo\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SQUARES = 40\n",
    "SQUARE_WIDTH = 0.1\n",
    "N_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geofileops_overlay(\n",
    "    left_df, right_df, input1_columns_prefix=\"l1_\", input2_columns_prefix=\"l2_\"\n",
    "):\n",
    "    temp_folder = tempfile.TemporaryDirectory()\n",
    "    left_df_file = str(Path(temp_folder.name, \"left.gpkg\"))\n",
    "    right_df_file = str(Path(temp_folder.name, \"right.gpkg\"))\n",
    "    union_file = str(Path(temp_folder.name, \"union.gpkg\"))\n",
    "\n",
    "    left_df.to_file(left_df_file)\n",
    "    right_df.to_file(right_df_file)\n",
    "\n",
    "    # This performs the same operation as geopandas overlay(how=\"union\")\n",
    "    gfo.union(\n",
    "        input1_path=left_df_file,\n",
    "        input2_path=right_df_file,\n",
    "        output_path=union_file,\n",
    "        input1_columns_prefix=input1_columns_prefix,\n",
    "        input2_columns_prefix=input2_columns_prefix,\n",
    "    )\n",
    "    overlay_data = gpd.read_file(union_file)\n",
    "\n",
    "    return overlay_data\n",
    "\n",
    "\n",
    "def geofileops_dissolve(df, groupby_columns):\n",
    "    # Create a temporary folder to save intermediate results on disk to\n",
    "    temp_folder = tempfile.TemporaryDirectory()\n",
    "    input_path = Path(temp_folder.name, \"input.gpkg\")\n",
    "    output_path = Path(temp_folder.name, \"output.gpkg\")\n",
    "\n",
    "    # Write the input data to disk\n",
    "    df.to_file(input_path)\n",
    "    print(f\"Original df {df}\")\n",
    "    # Dissolve\n",
    "    gfo.dissolve(\n",
    "        input_path=input_path,\n",
    "        output_path=output_path,\n",
    "        explodecollections=False,\n",
    "        groupby_columns=groupby_columns,\n",
    "    )\n",
    "\n",
    "    # Read the result and return\n",
    "    dissolved = gpd.read_file(output_path)\n",
    "    print(\"Dissolved df\")\n",
    "    print(dissolved)\n",
    "    return dissolved\n",
    "\n",
    "\n",
    "def merge_classified_polygons(\n",
    "    classified_polygons: gpd.GeoDataFrame,\n",
    "    class_column: str,\n",
    "    print_tiebreaking_stats: bool = False,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Take multiple potentially-overlapping polygons with associated class information and merge them\n",
    "    into non-overlapping classified polygons using the following strategy.\n",
    "    * For each location, compute the number of polygons that cast votes for each class\n",
    "    * For any regions that have a non-tied number of votes, set the output class to the majority vote\n",
    "    * For any regions with tied votes, break ties in favor of the less common class (based on un-ambigous regions)\n",
    "\n",
    "    Args:\n",
    "        classified_polygons (gpd.GeoDataFram): A geodataframe containing the `class_column` column\n",
    "        class_column (str): The column to use as the class\n",
    "        print_tiebreaking_stats (bool, optional): Print the fraction of area that needed to be tiebroken\n",
    "    \"\"\"\n",
    "    # Create a dictionary where the keys are the classes and the values are the dataframe with all\n",
    "    # the (multi)polygons corresponding to that class\n",
    "    grouped_by_class = {k[0]: v for k, v in classified_polygons.groupby([class_column])}\n",
    "    unique_classes = list(grouped_by_class.keys())\n",
    "\n",
    "    # Now, for each individual class, compute how many overlapping polygons there are for each\n",
    "    # location. This information will be used in future steps to prioritize that class if there are\n",
    "    # multiple predictions for the same location.\n",
    "    n_overlapping_per_class = []\n",
    "    for cls, geometries_per_class in grouped_by_class.items():\n",
    "        # Create a new column for counts that is all ones\n",
    "        geometries_per_class[\"counts\"] = 1\n",
    "        # Only retain the geometry column and the counts\n",
    "        geometries_per_class = geometries_per_class[[\"geometry\", \"counts\"]]\n",
    "\n",
    "        # Initialize the number overlapping to the first geometry, which has a count of 1\n",
    "        n_overlapping = geometries_per_class.iloc[0:1]\n",
    "        # Iterate over the subsequent geometries\n",
    "        for i in range(1, len(geometries_per_class)):\n",
    "            geometry = geometries_per_class[i : i + 1]\n",
    "            # Overlay the running total of overlaps with the new geometry\n",
    "            n_overlapping = geofileops_overlay(n_overlapping, geometry)\n",
    "            # Since we use the union approach, there will be some nan values for counts where one\n",
    "            # side of the overlay did not overlap the other one. Set these to zero.\n",
    "            n_overlapping.fillna(0, inplace=True)\n",
    "            # Add the counts from the left and right dataframe and set them to a new \"counts\" column\n",
    "            n_overlapping[\"counts\"] = (\n",
    "                n_overlapping[\"l1_counts\"] + n_overlapping[\"l2_counts\"]\n",
    "            )\n",
    "            # Since we'll be repeating this process, it's important to drop these columns or there\n",
    "            # will be a name collision on the next iteration\n",
    "            n_overlapping.drop([\"l1_counts\", \"l2_counts\"], axis=1, inplace=True)\n",
    "\n",
    "        # TODO, consider whether it would be more efficient to dissolve after every operation. It\n",
    "        # would be an additional step, but it could speed up the overlay\n",
    "        n_overlapping = geofileops_dissolve(n_overlapping, groupby_columns=\"counts\")\n",
    "        # Rename the counts column to the name of the class\n",
    "        n_overlapping.rename(columns={\"counts\": str(cls)}, inplace=True)\n",
    "        # Append to the running list\n",
    "        n_overlapping_per_class.append(n_overlapping)\n",
    "\n",
    "    # Overlay the votes for each class to get all the regions with distinct combinations of votes\n",
    "    votes_per_class = n_overlapping_per_class[0]\n",
    "    for single_class_overlay in n_overlapping_per_class[1:]:\n",
    "        print(\"About to overlap\")\n",
    "        print(votes_per_class)\n",
    "        print(\"With\")\n",
    "        print(single_class_overlay)\n",
    "        print(\"-----\")\n",
    "        votes_per_class = geofileops_overlay(votes_per_class, single_class_overlay, input1_columns_prefix=\"\", input2_columns_prefix=\"\")\n",
    "\n",
    "        print(\"Overlaid version is\")\n",
    "        print(votes_per_class)\n",
    "\n",
    "    # Similar to before, since we're doing a \"union\" overlay, there will be rows that don't have\n",
    "    # values for all columns. Fill them in with zero.\n",
    "    votes_per_class.fillna(0, inplace=True)\n",
    "\n",
    "    unique_classes_as_strs = [str(x) for x in unique_classes]\n",
    "    # Extract the counts columns to a numpy array and find the most common class per row\n",
    "    class_counts_matrix = votes_per_class[unique_classes_as_strs].values\n",
    "    max_class_counts = np.max(class_counts_matrix, axis=1, keepdims=True)\n",
    "\n",
    "    # Find rows where one class has the most votes (there are no ties)\n",
    "    one_max_class = np.sum(max_class_counts == class_counts_matrix, axis=1) == 1\n",
    "\n",
    "    # Extract rows where one class has the most votes\n",
    "    rows_with_one_class = votes_per_class.iloc[one_max_class]\n",
    "    # Label them with the max class\n",
    "    rows_with_one_class[\"max_class\"] = rows_with_one_class[unique_classes_as_strs].idxmax(\n",
    "        axis=1\n",
    "    )\n",
    "    # Dissolve all polygons so we have one (multi)polygon per class\n",
    "    rows_with_one_class = rows_with_one_class.dissolve(\"max_class\", as_index=False)\n",
    "\n",
    "    # Compute the area of each\n",
    "    rows_with_one_class[\"area\"] = rows_with_one_class.area\n",
    "\n",
    "    # Order the classes from smallest area to largest, based on unambigous regions\n",
    "    sorted_inds = (rows_with_one_class[\"area\"]).argsort()\n",
    "    sorted_classes = rows_with_one_class.index[sorted_inds].tolist()\n",
    "\n",
    "    if print_tiebreaking_stats:\n",
    "        area_of_sorted = rows_with_one_class.dissolve().area[0]\n",
    "        total_area = votes_per_class.dissolve().area[0]\n",
    "\n",
    "        print(\n",
    "            f\"Ties had to be broken for {(100 *(1 - (area_of_sorted/total_area))):.1f}% of the total predictions\"\n",
    "        )\n",
    "\n",
    "    # Determine which classes (if any) have no non-overlapping regions. Add them to the start of the\n",
    "    # list\n",
    "    zero_area_classes = [c for c in unique_classes_as_strs if c not in sorted_classes]\n",
    "    # Prepend the classes to the beginning of the list of sorted classes\n",
    "    sorted_classes = zero_area_classes + sorted_classes\n",
    "    print(sorted_classes)\n",
    "    print(type(sorted_classes[0]))\n",
    "\n",
    "    # Reorder the columns starting with the rarest class. Then compute the index of the max value.\n",
    "    # Since this returns the first instance of the maximum value, ties will be broken in favor of\n",
    "    # the class that had the least area in the unambigious region.\n",
    "    max_class = votes_per_class[sorted_classes].idxmax(axis=1)\n",
    "    # Create a new column for the max class\n",
    "    votes_per_class[\"max_class\"] = max_class\n",
    "    votes_per_class = votes_per_class[[\"max_class\", \"geometry\"]]\n",
    "    # Dissolve so there's only one (multi)polygon per class\n",
    "    votes_per_class = votes_per_class.dissolve(\"max_class\", as_index=False)\n",
    "    return votes_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_corners = np.random.random((N_SQUARES, 2)) * (1 - SQUARE_WIDTH)\n",
    "boxes = [shapely.box(tl[0], tl[1], tl[0] + SQUARE_WIDTH, tl[1] + SQUARE_WIDTH) for tl in tl_corners]\n",
    "classes = np.random.randint(0, N_CLASSES, size=N_SQUARES)\n",
    "classes = np.array([\"abcd\"[i] for i in classes])\n",
    "\n",
    "classified_polygons = gpd.GeoDataFrame({\"geometry\": boxes, \"classes\": classes}, crs=3311)\n",
    "classified_polygons.plot(\"classes\",alpha=0.8, cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge_classified_polygons(classified_polygons=classified_polygons, class_column=\"classes\")\n",
    "merged.plot(\"max_class\", cmap=\"tab10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geofileops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
